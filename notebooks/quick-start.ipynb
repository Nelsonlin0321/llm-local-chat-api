{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/nvme1n1p2/huggingface-models/baichuan-inc-Baichuan2-13B-Chat-4bits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "    revision=\"v2.0\",\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.half()` is not supported for quantized model. Please use the model as it is, since the model has already been casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/mnt/nvme1n1p2/clive/miniconda/lib/python3.11/site-packages/transformers/modeling_utils.py:2600\u001b[0m, in \u001b[0;36mPreTrainedModel.half\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhalf\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;66;03m# Checks if the model is quantized\u001b[39;00m\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2600\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2601\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.half()` is not supported for quantized model. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2602\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2603\u001b[0m         )\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2605\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mhalf(\u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mValueError\u001b[0m: `.half()` is not supported for quantized model. Please use the model as it is, since the model has already been casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    revision=\"v2.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True).half().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config = GenerationConfig.from_pretrained(model_path, revision=\"v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\"role\": \"user\", \"content\": \"解释一下“温故而知新”\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"温故而知新\"是一句源自《论语》的古文名言，它意味着通过回顾过去的学习和经验，我们可以从中汲取新的知识和智慧。这句话鼓励我们在学习过程中不断地复习和巩固已有的知识，同时保持开放的心态去接受新的观念和方法。这样，我们既能更好地理解和掌握已有的知识，又能适应不断变化的世界，从而实现持续的成长和发展。\n"
     ]
    }
   ],
   "source": [
    "# messages = []\n",
    "# messages.append(example)\n",
    "# response = model.chat(tokenizer, messages)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"温故而知新\"是一句源自《论语》的古文，这句话的意思是回顾过去，从中汲取经验教训，从而更好地理解现在和预见未来。这句话鼓励我们在学习、工作和生活中，要经常回顾过去的经历，从中吸取宝贵的经验和教训，以便更好地应对现在的挑战和问题，并为未来的发展做好准备。\n",
      "\n",
      "\"温故\"指的是回顾过去，\"知新\"则表示从过去的经验中获取新的知识和智慧。整句话强调的是我们要善于总结历史，从中汲取力量，以便更好地面对现实和未来。\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append(example)\n",
    "with torch.no_grad():\n",
    "    response = model.chat(tokenizer, messages)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"温故而知新\"是《论语》中的名言，意思是通过学习过去，可以更好地理解现在和未来。\n"
     ]
    }
   ],
   "source": [
    "messages.append({'role':\"assistant\",\"content\":response})\n",
    "messages.append({'role':\"user\",\"content\":\"把答案简短一点\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '解释一下“温故而知新”'},\n",
       " {'role': 'assistant',\n",
       "  'content': '“温故而知新”是一句源自《论语》的古语，这句话的意思是回顾过去，从中汲取经验教训，从而更好地理解现在和未来。这句话鼓励我们在生活中不断地学习和成长，通过回顾过去的经历和知识，我们可以发现新的观点和见解，从而更好地应对当前和未来的挑战。总的来说，“温故而知新”是一种积极的人生态度，它强调从过去中汲取智慧，以便更好地面对现在的需求和未来的发展。'},\n",
       " {'role': 'user', 'content': '把答案简短一点'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method chat in module transformers_modules.baichuan-inc-Baichuan2-13B-Chat-4bits.modeling_baichuan:\n",
      "\n",
      "chat(tokenizer, messages: List[dict], stream=False, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None) method of transformers_modules.baichuan-inc-Baichuan2-13B-Chat-4bits.modeling_baichuan.BaichuanForCausalLM instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d8e01e88-2c85-4cb5-82ca-9e8d6480e291'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
