{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_with_most_memory():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count == 0:\n",
    "        print(\" GPUs available.\")\n",
    "        return None\n",
    "\n",
    "    min_memory_allocated=0\n",
    "    max_memory_device = None\n",
    "\n",
    "    for i in range(device_count):\n",
    "        device = f\"cuda:{i}\"\n",
    "        memory_allocated = torch.cuda.memory_allocated(device)\n",
    "        if min_memory_allocated >= memory_allocated:\n",
    "            min_memory_allocated = memory_allocated\n",
    "            max_memory_device = device\n",
    "\n",
    "    return max_memory_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_gpu_with_most_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/nvme1n1p2/huggingface-models/mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a383642d4ada441d969ce30eff58a2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device).half().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"tell me a story\"\n",
    "    }\n",
    "    ]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793,  1912,   528,   264,  2838,   733, 28748,\n",
       "         16289, 28793]], device='cuda:1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=32*1000, do_sample=True,pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] tell me a story [/INST] Once upon a time, in a far-off land nestled between rolling hills and the sparkling blue sea, there was a tiny village named Serenity. This quaint little place was renowned for its beautiful sunsets, enchanting melodies of crickets in the evenings, and the sweet aroma of jasmine flowers that carried on the breeze. The villagers were simple-living, contented people who cherished the gifts that nature bestowed upon them. Their days were filled with hard work and laughter, and as the sun set, they would gather around the village square to share stories and enjoy a hearty meal.\n",
      "\n",
      "Amongst the villagers, there was a young girl named Lila. She was known for her boundless curiosity and her love for animals. Lila had a peculiar friend, a magnificent eagle named Gideon. They had made friends when Lila was but a toddler. She would leave out scraps of food for him in their courtyard, and in turn, he would grace her with his awe-inspiring presence from the treetops nearby. Over time, they had formed an unbreakable bond, and Lila would often ask Gideon about the vast expanse of the world beyond Serenity.\n",
      "\n",
      "One fateful day, Lila found an ancient map in her Grandfather's attic. The map was worn and frayed, and it seemed to depict the location of a hidden treasure, buried deep in the heart of the Rainforest of Mysteries. Lila's heart jumped with excitement as she shared the news with Gideon. He agreed to be her faithful companion and guide on this new adventure.\n",
      "\n",
      "Together, Lila and Gideon embarked on an incredible journey, following the clues on the map through lush, green forests, across roaring rivers, and over towering mountains. They encountered many challenges and obstacles, but they faced them with courage, determination, and a steadfast belief in each other. As they journeyed deeper into the wilderness, they discovered breathtaking landscapes, met new friends, and learned valuable lessons about friendship, teamwork, and the true meaning of adventure.\n",
      "\n",
      "Finally, after many weeks, they arrived at a hidden clearing, where the sun shone down upon the most beautiful sight Lila had ever seen – a magnificent waterfall cascading into a crystal-clear lake, surrounded by vibrant, exotic flowers of every color. There, at the heart of the waterfall, they discovered the treasure – a magnificent chest filled with glittering gems, rare spices, and golden coins.\n",
      "\n",
      "But the true treasure was not in the riches they had found; it was in the experiences they had had, the memories they had made, and the unbreakable bond they had forged. They returned to their beloved village of Serenity with their heads held high, knowing that they had truly discovered something far more valuable than gold and jewels – the magic of adventure and the everlasting connection between a girl and her eagle friend.\n",
      "\n",
      "And from that day on, the story of Lila and Gideon became the stuff of legend in their village, a testament to the power of friendship, the wonders of the world, and the indomitable spirit of adventure that lives within each one of us.</s>\n"
     ]
    }
   ],
   "source": [
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.findall(r'</s>(.*?)</s>', response)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Wen fa er zhin\" is a famous quote from \"The Analects,\" encouraging us to gain new knowledge by reflecting on past experiences. It urges us to continually learn and grow by revisiting the past and finding new insights.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.split(\"[/INST]\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPU with the most available memory is cuda:1.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_gpu_with_most_memory():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count == 0:\n",
    "        print(\" GPUs available.\")\n",
    "        return None\n",
    "\n",
    "    min_memory_allocated=0\n",
    "    max_memory_device = None\n",
    "\n",
    "    for i in range(device_count):\n",
    "        device = torch.device(f\"cuda:{i}\")\n",
    "        memory_allocated = torch.cuda.memory_allocated(device)\n",
    "        if min_memory_allocated >= memory_allocated:\n",
    "            min_memory_allocated = memory_allocated\n",
    "            max_memory_device = device\n",
    "\n",
    "    return max_memory_device\n",
    "\n",
    "# Example usage\n",
    "gpu_with_most_memory = get_gpu_with_most_memory()\n",
    "if gpu_with_most_memory is not None:\n",
    "    print(f\"The GPU with the most available memory is {gpu_with_most_memory}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_with_most_memory = get_gpu_with_most_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_with_most_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'./../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c04294fe674969ae1aad27855a8a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from app.routers.mistral_7b_instruct.model import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1845d9e633640aa98762b127d61d2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model(\"/mnt/nvme1n1p2/huggingface-models/mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"解释一下“温故而知新”\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"“温故而知新”是一句源自《论语》的古文名言，其含义是回顾过去以获取新的知识和领悟。这句话鼓励人们在学习和生活中不断地回顾和反思过去的经验，从而发现新的理解和启示。通过这种方式，人们可以不断成长和提高自己，使自己的知识、技能和认识水平得到提升。”\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"make the answer brief\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0f582ac9-9aa2-4dbd-bb82-68afaabd50b4',\n",
       " 'object': 'chat.completion',\n",
       " 'created': None,\n",
       " 'model': 'Mistral-7B-Instruct-v0.2',\n",
       " 'usage': {'prompt_tokens': None,\n",
       "  'completion_tokens': None,\n",
       "  'total_tokens': 230},\n",
       " 'choices': [{'message': {'role': 'assistant',\n",
       "    'content': '\"温故而知新\" is a famous quote from the \"Analects\" of Confucius. It means to gain new knowledge by reflecting on the past. This encourages people to continuously learn from their past experiences and discover new insights. Through this process, people can broaden their knowledge, skills, and understanding.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'index': 0}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_answer(model_name=\"Mistral-7B-Instruct-v0.2\",messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_with_most_memory():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count == 0:\n",
    "        print(\" GPUs available.\")\n",
    "        return None\n",
    "\n",
    "    min_memory_allocated=0\n",
    "    max_memory_device = None\n",
    "\n",
    "    for i in range(device_count):\n",
    "        device = f\"cuda:{i}\"\n",
    "        memory_allocated = torch.cuda.memory_allocated(device)\n",
    "        if min_memory_allocated >= memory_allocated:\n",
    "            min_memory_allocated = memory_allocated\n",
    "            max_memory_device = device\n",
    "\n",
    "    return max_memory_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpu_with_most_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_with_most_memory():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count == 0:\n",
    "        raise Exception(\"GPUs available.\")\n",
    "        \n",
    "\n",
    "    min_memory_allocated=float(\"inf\")\n",
    "    max_memory_device = None\n",
    "\n",
    "    for i in range(device_count):\n",
    "        device = f\"cuda:{i}\"\n",
    "        \n",
    "        memory_allocated = torch.cuda.memory_allocated(device)\n",
    "\n",
    "        if min_memory_allocated >= memory_allocated:\n",
    "            min_memory_allocated = memory_allocated\n",
    "            max_memory_device = device\n",
    "\n",
    "    return max_memory_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpu_with_most_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count == 0:\n",
    "    raise Exception(\"GPUs available.\")\n",
    "    \n",
    "\n",
    "min_memory_allocated=float(\"inf\")\n",
    "max_memory_device = None\n",
    "\n",
    "for i in range(device_count):\n",
    "    device = f\"cuda:{i}\"\n",
    "    \n",
    "    memory_allocated = torch.cuda.memory_allocated(device)\n",
    "\n",
    "\n",
    "    if min_memory_allocated >= memory_allocated:\n",
    "        min_memory_allocated = memory_allocated\n",
    "        max_memory_device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_memory_allocated >= memory_allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_memory_allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
