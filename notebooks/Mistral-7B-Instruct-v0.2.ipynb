{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/nvme1n1p2/huggingface-models/mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c954a716db624176a52bbce834eebdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device).half().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"解释一下“温故而知新”\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"“温故而知新”是一句源自《论语》的古文名言，其含义是回顾过去以获取新的知识和领悟。这句话鼓励人们在学习和生活中不断地回顾和反思过去的经验，从而发现新的理解和启示。通过这种方式，人们可以不断成长和提高自己，使自己的知识、技能和认识水平得到提升。”\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"make the answer brief\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793, 28705, 29386, 30334, 28969, 29061, 28835,\n",
       "         30971, 31063, 29746, 29661, 29056, 28838,   733, 28748, 16289, 28793,\n",
       "         28835, 30971, 31063, 29746, 29661, 29056, 28838, 28971, 28969, 30347,\n",
       "         29512, 29159, 30095, 30144, 29892, 30028, 28914, 30313, 29019, 29046,\n",
       "         30065, 28924, 29450, 29872, 29476, 28971, 29034,   236,   164,   193,\n",
       "         29202, 29651, 29074, 26257, 29056, 28914, 29661, 30093, 29131, 30610,\n",
       "           233,   133,   162, 28944, 29176, 30347, 29456,   236,   191,   150,\n",
       "         31613, 29086, 29550, 29010, 29500, 30978, 29131, 29128, 29785, 28991,\n",
       "         28988, 29396, 29146, 29034,   236,   164,   193, 29131, 30004, 30450,\n",
       "         29202, 29651, 28914, 29464, 29502, 28924, 29505, 29746, 29099, 29419,\n",
       "         29056, 28914, 29093, 29386, 29131, 29625, 29073, 28944, 29217, 29202,\n",
       "         29176, 29824, 29082, 29122, 28924, 29086, 29550, 29052, 29074, 28988,\n",
       "         29396, 29025, 29495, 29131, 29279, 29366, 29159, 29957, 28924, 29154,\n",
       "         29159, 29957, 28914, 29661, 30093, 29041, 30021, 29084, 29131, 29262,\n",
       "         30093, 29692, 29549, 29291, 29036, 29279, 30401, 28944, 28838,     2,\n",
       "           733, 16289, 28793,  1038,   272,  4372,  6817,   733, 28748, 16289,\n",
       "         28793]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=32*1000, do_sample=True,pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] 解释一下“温故而知新” [/INST]“温故而知新”是一句源自《论语》的古文名言，其含义是回顾过去以获取新的知识和领悟。这句话鼓励人们在学习和生活中不断地回顾和反思过去的经验，从而发现新的理解和启示。通过这种方式，人们可以不断成长和提高自己，使自己的知识、技能和认识水平得到提升。”</s> [INST] make the answer brief [/INST] \"温故而知新\" is a famous saying from the \"Analects\" in Chinese philosophy. It means exploring new knowledge by reflecting on past experiences. It encourages continually reviewing past knowledge to gain new insights. This approach promotes growth and increased understanding.</s>\n"
     ]
    }
   ],
   "source": [
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.findall(r'</s>(.*?)</s>', response)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Wen fa er zhin\" is a famous quote from \"The Analects,\" encouraging us to gain new knowledge by reflecting on past experiences. It urges us to continually learn and grow by revisiting the past and finding new insights.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.split(\"[/INST]\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPU with the most available memory is cuda:1.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_gpu_with_most_memory():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count == 0:\n",
    "        print(\" GPUs available.\")\n",
    "        return None\n",
    "\n",
    "    min_memory_allocated=0\n",
    "    max_memory_device = None\n",
    "\n",
    "    for i in range(device_count):\n",
    "        device = torch.device(f\"cuda:{i}\")\n",
    "        memory_allocated = torch.cuda.memory_allocated(device)\n",
    "        if min_memory_allocated >= memory_allocated:\n",
    "            min_memory_allocated = memory_allocated\n",
    "            max_memory_device = device\n",
    "\n",
    "    return max_memory_device\n",
    "\n",
    "# Example usage\n",
    "gpu_with_most_memory = get_gpu_with_most_memory()\n",
    "if gpu_with_most_memory is not None:\n",
    "    print(f\"The GPU with the most available memory is {gpu_with_most_memory}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_with_most_memory = get_gpu_with_most_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_with_most_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'./../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c04294fe674969ae1aad27855a8a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from app.routers.mistral_7b_instruct.model import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1845d9e633640aa98762b127d61d2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model(\"/mnt/nvme1n1p2/huggingface-models/mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"解释一下“温故而知新”\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"“温故而知新”是一句源自《论语》的古文名言，其含义是回顾过去以获取新的知识和领悟。这句话鼓励人们在学习和生活中不断地回顾和反思过去的经验，从而发现新的理解和启示。通过这种方式，人们可以不断成长和提高自己，使自己的知识、技能和认识水平得到提升。”\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"make the answer brief\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0f582ac9-9aa2-4dbd-bb82-68afaabd50b4',\n",
       " 'object': 'chat.completion',\n",
       " 'created': None,\n",
       " 'model': 'Mistral-7B-Instruct-v0.2',\n",
       " 'usage': {'prompt_tokens': None,\n",
       "  'completion_tokens': None,\n",
       "  'total_tokens': 230},\n",
       " 'choices': [{'message': {'role': 'assistant',\n",
       "    'content': '\"温故而知新\" is a famous quote from the \"Analects\" of Confucius. It means to gain new knowledge by reflecting on the past. This encourages people to continuously learn from their past experiences and discover new insights. Through this process, people can broaden their knowledge, skills, and understanding.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'index': 0}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_answer(model_name=\"Mistral-7B-Instruct-v0.2\",messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
